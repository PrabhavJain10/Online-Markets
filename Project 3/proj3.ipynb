{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Outcomes from No-regret Learning in Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Algorithm Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWAlg:\n",
    "    def __init__(self, epsilon, k, h, myBids, myValue):\n",
    "        self.weights = np.ones(k)\n",
    "        self.payoffs = np.zeros(k)\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.sumWeights = np.sum(self.weights)\n",
    "        self.probs = self.weights/self.sumWeights\n",
    "        self.myValue = myValue\n",
    "        self.myBids = myBids\n",
    "\n",
    "    def getBids(self):\n",
    "        return self.myBids\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.myValue\n",
    "\n",
    "    def getAction(self):\n",
    "        j = np.random.choice(self.k, 1, p = self.probs)\n",
    "        return j.item()\n",
    "    \n",
    "    def update(self, payoffs):\n",
    "        for j in range(len(payoffs)):\n",
    "            curPayoff = payoffs[j]\n",
    "            self.payoffs[j] = self.payoffs[j] + curPayoff\n",
    "            newWeight = (1+self.epsilon)**(self.payoffs[j]/self.h)\n",
    "            self.weights[j] = newWeight\n",
    "        self.sumWeights = np.sum(self.weights)\n",
    "        if(self.epsilon > 1):\n",
    "            self.weights = self.weights/self.sumWeights\n",
    "            self.sumWeights = np.sum(self.weights)\n",
    "        self.probs = self.weights/self.sumWeights\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstPriceReserve:\n",
    "    # bidders have to be ordered the same way every time\n",
    "    # reserve given as a \n",
    "    def __init__(self, bids, reserve):\n",
    "        self.numBidders = bids.size\n",
    "        self.totalPayoffs = np.zeros(self.numBidders)\n",
    "        self.bids = bids\n",
    "        self.reserve = reserve\n",
    "        \n",
    "    def generate(self):\n",
    "        winningBid = 0\n",
    "        winner = -1\n",
    "        tiedBidders = []\n",
    "        # check bids of all bidders\n",
    "        for count, bid in enumerate(self.bids):\n",
    "            if bid > winningBid:\n",
    "                winningBid = bid\n",
    "                winner = count\n",
    "            elif (bid == winningBid) and (winner != -1):\n",
    "                tiedBidders.add(count)\n",
    "        winner = random.choice(tiedBidders)\n",
    "        # see if winning bid is greater than reserve price\n",
    "        if (winningBid < self.reserve): \n",
    "            winningBid = self.reserve\n",
    "            winner = -1\n",
    "        return winningBid, winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mapBidToPayoffs:\n",
    "    def __init__(self, myValue, possibleBids):\n",
    "        self.myValue = myValue\n",
    "        self.possibleBids = possibleBids\n",
    "    \n",
    "    def generatePayoffs(self, winningBid):\n",
    "        payoffs = np.zeros(self.possibleBids.size)\n",
    "        for count, bid in enumerate(self.possibleBids):\n",
    "            if bid >= winningBid:\n",
    "                payoffs[count] = bid - self.myValue\n",
    "            else:\n",
    "                payoffs[count] = 0\n",
    "        return payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPlayers = 2\n",
    "epsilons = [2,5]\n",
    "players = []\n",
    "h = 1\n",
    "k = 10\n",
    "stepSize = 1/k * (np.log(h))\n",
    "for count in range(numPlayers):\n",
    "    # pick a distribution and numPlayers values from it\n",
    "    playerValue = random.uniform(0,h)\n",
    "    # create possible bids using geometric discretization\n",
    "    playerBids = []\n",
    "    for j in range(k):\n",
    "        playerBids.append(playerValue - (1 + stepSize)**(j+1))\n",
    "    # create player\n",
    "    player = EWAlg(epsilons[count], k, h, playerBids, playerValue)\n",
    "    players.append(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo(numTrials, payoffGenerator, epsilon, k, h, n):\n",
    "    avgFinalPayoff = 0\n",
    "    avgRegretPerRound = [[] for i in range(numTrials)]\n",
    "    for trial in range(numTrials):\n",
    "        alg = EWAlg(epsilon, k, h)\n",
    "        finalPayoff = 0\n",
    "        actionPayoffs = np.zeros(k)\n",
    "        generator = payoffGenerator(k)\n",
    "        regretPerRound = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            payoffs = generator.generate()\n",
    "            j = alg.getAction()\n",
    "            myPayoff = payoffs[j]\n",
    "            actionPayoffs += payoffs\n",
    "            alg.update(payoffs)\n",
    "            finalPayoff += myPayoff\n",
    "            OPT = max(actionPayoffs)\n",
    "            regret = (OPT - finalPayoff).item() / (i+1)\n",
    "            regretPerRound[i] = regret\n",
    "        avgFinalPayoff += finalPayoff\n",
    "        avgRegretPerRound[trial] = regretPerRound\n",
    "    return avgFinalPayoff/numTrials, np.mean(avgRegretPerRound, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloTrackActions(numTrials, payoffGenerator, epsilon, k, h, n):\n",
    "    avgFinalPayoff = 0\n",
    "    avgRegretPerRound = [[] for i in range(numTrials)]\n",
    "    actionTrial = []\n",
    "    for trial in range(numTrials):\n",
    "        alg = EWAlg(epsilon, k, h)\n",
    "        finalPayoff = 0\n",
    "        actionPayoffs = np.zeros(k)\n",
    "        generator = payoffGenerator(k)\n",
    "        regretPerRound = np.zeros(n)\n",
    "        actions = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            payoffs = generator.generate()\n",
    "            j = alg.getAction()\n",
    "            actions[i] = j\n",
    "            myPayoff = payoffs[j]\n",
    "            actionPayoffs += payoffs\n",
    "            alg.update(payoffs)\n",
    "            finalPayoff += myPayoff\n",
    "            OPT = max(actionPayoffs)\n",
    "            regret = (OPT - finalPayoff).item() / (i+1)\n",
    "            regretPerRound[i] = regret\n",
    "        actionTrial.append(actions)\n",
    "        avgFinalPayoff += finalPayoff\n",
    "        avgRegretPerRound[trial] = regretPerRound\n",
    "    return avgFinalPayoff/numTrials, np.mean(avgRegretPerRound, axis=0), np.array(actionTrial)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Fair Payoffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each round i:\n",
    "\n",
    "Draw a payoff x ~ U[0,1] (i.e., from the uniform distribution on interval [0,1])\n",
    "\n",
    "Assign this payoff to the action j* that has the smallest total payoff so far, i.e., j* = argminj Vji-1 where Vji = Î£ir=1 vji. \n",
    "(All other actions get 0 payoff in round i.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99, -0.5, 0, 0.1, 0.1517, 0.2, 0.4, 0.7, 1, 2, 10, 1000]\n",
      "[12.572066468624602, 6.089115420080595, 4.952127590364313, 4.888985005026019, 4.662385648248368, 4.658095671678926, 4.491083295527148, 4.237502645046786, 3.988782430758382, 3.551178754492665, 2.1475778776821812, 0.3206374990307352]\n",
      "[-0.07164880805723829, -0.0066178097291542375, 0.0044772387027638095, 0.005445673155832275, 0.007507744684371933, 0.007499525986132154, 0.0091217183309684, 0.011695349045173858, 0.014306945820161048, 0.01850769358056152, 0.03260957846953832, 0.05091754046141372]\n"
     ]
    }
   ],
   "source": [
    "h = 1 # fixed\n",
    "# hyperparameters\n",
    "k = 10\n",
    "n = 100\n",
    "epsilons = [-0.99, -0.5, 0, 0.1, 0.1517, 0.2, 0.4, 0.7, 1, 2, 10, 1000] # to be studied\n",
    "monteCarloBound = 1000\n",
    "\n",
    "AFEpsilonPayoffs = []\n",
    "AFEpsilonRegretPerRound = []\n",
    "for epsilon in epsilons:\n",
    "    finalPayoff, regretPerRound = MonteCarlo(monteCarloBound, AdversarialFair, epsilon, k, h, n)\n",
    "    AFEpsilonPayoffs.append(finalPayoff)\n",
    "    AFEpsilonRegretPerRound.append(regretPerRound)\n",
    "\n",
    "AFEpsilonAvgRegrets = [i[99] for i in AFEpsilonRegretPerRound]\n",
    "print(epsilons)\n",
    "print(AFEpsilonPayoffs)\n",
    "print(AFEpsilonAvgRegrets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
